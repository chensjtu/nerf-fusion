{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vis depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = torch.load(\"gt.pt\")\n",
    "re = torch.load('rend_dp.pt')\n",
    "img = torch.cat((gt, re, torch.abs(gt-re)),dim=-1).numpy()\n",
    "plt.imshow(img,cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test backproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_project_points_cam_coords(intrinsic, uvd, c2w):\n",
    "    '''\n",
    "    As inv_project_points but doesn't do the homogeneous transformation\n",
    "    int: 3x3, uvd:Nx3, c2w:3X4\n",
    "    '''\n",
    "    if len(uvd.shape) > 2:\n",
    "        uvd = uvd.reshape(-1,3)\n",
    "    if intrinsic.shape[0]>3:\n",
    "        intrinsic = intrinsic[:3,:3]\n",
    "    if c2w.shape[0]>3:\n",
    "        c2w = c2w[:3,:]\n",
    "    n_points = uvd.shape[0]\n",
    "\n",
    "    # creating the camera rays\n",
    "    uv1 = torch.hstack((uvd[:, :2], torch.ones((n_points, 1))))\n",
    "    camera_rays = torch.mm(intrinsic.inverse(), uv1.T).T\n",
    "\n",
    "    # forming the xyz points in the camera coordinates\n",
    "    temp = uvd[:, 2][:,None].repeat(1,3)\n",
    "    xyz_at_cam_loc = temp * camera_rays\n",
    "    xyz_at_cam_loc = torch.hstack((xyz_at_cam_loc, torch.ones((n_points, 1))))\n",
    "    # project to xyz\n",
    "    xyz_world = torch.mm(c2w, xyz_at_cam_loc.T).T\n",
    "\n",
    "    return xyz_world\n",
    "\n",
    "# @torch.jit.script\n",
    "def re_projection(uvd, intrinsic, c2w):\n",
    "    '''\n",
    "    As inv_project_points but doesn't do the homogeneous transformation\n",
    "    int: 3x3, uvd:Nx3, c2w:3X4\n",
    "    '''\n",
    "    if len(uvd.shape) > 2:\n",
    "        uvd = uvd.reshape(-1,3)\n",
    "    if intrinsic.shape[0]>3:\n",
    "        intrinsic = intrinsic[:3,:3]\n",
    "    if c2w.shape[0]>3:\n",
    "        c2w = c2w[:3,:]\n",
    "    n_points = uvd.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def projection(xyz, K, ext):\n",
    "    newK = torch.eye(4)\n",
    "    newK[:3,:3] = K\n",
    "    P = newK@ext\n",
    "    uvz = P@xyz\n",
    "    return uvz, uvz[:2,:]/uvz[2:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[300,0,400],[0,300,400],[0,0,1]]).float()\n",
    "R = torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,1,-1],[0,0,0,1]]).float()\n",
    "print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pw = torch.tensor([[3,4,4,1]]).float().T\n",
    "Pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection(Pw,K,pose.inverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "a = torch.zeros((1))\n",
    "b = torch.ones((1))\n",
    "isinstance((a,b), Tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = inv_project_points_cam_coords(K,torch.tensor([682.8427,682.8427,3]).unsqueeze(0),pose)\n",
    "wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = inv_project_points_cam_coords(K,torch.tensor([580,640,5]).unsqueeze(0),pose)\n",
    "wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pc_tmp = torch.ones((4,1))\n",
    "Pc_tmp[:3,] = Pc_tmp\n",
    "pose[:3,:]@Pc_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pc_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.inverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test f.embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "pix_mask = torch.ones((9,16)).long()[0]\n",
    "feats = torch.randn((9,16,64))[0]\n",
    "pix_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([0, 1, 0])\n",
    "x = torch.tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
    "\n",
    "nrow = torch.unique(labels).size(0)\n",
    "ncol = x.size(1)\n",
    "out = torch.zeros((nrow, ncol), dtype=x.dtype)\n",
    "out.index_add_(0, labels, x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([0, 2, 1])\n",
    "x = torch.tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\n",
    "\n",
    "nrow = torch.unique(labels).size(0)\n",
    "ncol = x.size(1)\n",
    "out = torch.zeros((nrow, ncol), dtype=x.dtype)\n",
    "out.index_add_(0, labels, x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf_info = \"/home/yangchen/projects/SLAM_systems/Accel-RF/demo_dir/scannet/all_tsdf_9/scene0079_00/tsdf_info.pkl\"\n",
    "fragments = \"/data/yangchen/datasets_in_papers/neural_RGBD_recon/all_tsdf/breakfast_room/fragments.pkl\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fragments, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scene': 'breakfast_room',\n",
       "  'fragment_id': 0,\n",
       "  'image_ids': [0, 8, 16, 23, 30, 38, 46, 53, 61],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 1,\n",
       "  'image_ids': [62, 70, 78, 86, 93, 100, 108, 116, 124],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 2,\n",
       "  'image_ids': [125, 133, 140, 149, 157, 164, 172, 180, 187],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 3,\n",
       "  'image_ids': [188, 195, 202, 210, 217, 224, 231, 239, 247],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 4,\n",
       "  'image_ids': [248, 257, 263, 291, 300, 307, 314, 320, 326],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 5,\n",
       "  'image_ids': [327, 333, 339, 345, 351, 357, 364, 372, 379],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 6,\n",
       "  'image_ids': [380, 387, 394, 401, 409, 417, 425, 433, 443],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 7,\n",
       "  'image_ids': [444, 452, 460, 468, 475, 482, 489, 496, 503],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 8,\n",
       "  'image_ids': [504, 512, 520, 527, 534, 541, 547, 554, 561],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 9,\n",
       "  'image_ids': [562, 569, 576, 584, 593, 603, 613, 621, 629],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 10,\n",
       "  'image_ids': [630, 637, 644, 651, 658, 665, 673, 681, 689],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 11,\n",
       "  'image_ids': [690, 698, 705, 712, 720, 729, 737, 743, 750],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 12,\n",
       "  'image_ids': [751, 757, 763, 769, 776, 785, 824, 835, 844],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 13,\n",
       "  'image_ids': [845, 853, 860, 867, 874, 880, 886, 892, 898],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 14,\n",
       "  'image_ids': [899, 905, 910, 916, 922, 928, 933, 939, 945],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 15,\n",
       "  'image_ids': [946, 952, 959, 965, 971, 979, 986, 993, 1003],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 16,\n",
       "  'image_ids': [1004, 1018, 1027, 1035, 1043, 1050, 1059, 1066, 1072],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04},\n",
       " {'scene': 'breakfast_room',\n",
       "  'fragment_id': 17,\n",
       "  'image_ids': [1073, 1081, 1088, 1094, 1100, 1108, 1116, 1122, 1130],\n",
       "  'vol_origin': array([-7.0800014, -5.582102 , -5.5077543], dtype=float32),\n",
       "  'voxel_size': 0.04}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = []\n",
    "for i in range(len(data)):\n",
    "    tmp.extend(data[i]['image_ids'])\n",
    "len(tmp)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def get_points_vox(center_pts: torch.Tensor, voxel_size: float=0.04, pt_pervox:int=8):\n",
    "    '''\n",
    "        input: center_pts: N, 3\n",
    "        sample_pts: N, 512, 3\n",
    "    '''\n",
    "    xv, yv, zv = torch.meshgrid(\n",
    "        torch.arange(0, pt_pervox),\n",
    "        torch.arange(0, pt_pervox),\n",
    "        torch.arange(0, pt_pervox),\n",
    "    )\n",
    "    vox_coords = torch.stack([xv.flatten(), yv.flatten(), zv.flatten()], dim=1).long().to(center_pts.device) # 512, 3\n",
    "    sample_pts = (center_pts - voxel_size*0.5)[:,None] + (vox_coords*voxel_size/pt_pervox).unsqueeze(0)\n",
    "    return sample_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(8).reshape(2,2,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 1],\n",
       "         [2, 3, 2, 3],\n",
       "         [0, 1, 0, 1],\n",
       "         [2, 3, 2, 3]],\n",
       "\n",
       "        [[4, 5, 4, 5],\n",
       "         [6, 7, 6, 7],\n",
       "         [4, 5, 4, 5],\n",
       "         [6, 7, 6, 7]],\n",
       "\n",
       "        [[0, 1, 0, 1],\n",
       "         [2, 3, 2, 3],\n",
       "         [0, 1, 0, 1],\n",
       "         [2, 3, 2, 3]],\n",
       "\n",
       "        [[4, 5, 4, 5],\n",
       "         [6, 7, 6, 7],\n",
       "         [4, 5, 4, 5],\n",
       "         [6, 7, 6, 7]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.repeat(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0],\n",
       "          [0, 0, 1]],\n",
       "\n",
       "         [[0, 1, 0],\n",
       "          [0, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0],\n",
       "          [1, 0, 1]],\n",
       "\n",
       "         [[1, 1, 0],\n",
       "          [1, 1, 1]]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv, yv, zv = torch.meshgrid(\n",
    "    torch.arange(0, 2),\n",
    "    torch.arange(0, 2),\n",
    "    torch.arange(0, 2),\n",
    ")\n",
    "vox_coords = torch.stack([xv.flatten(), yv.flatten(), zv.flatten()], dim=1).long()\n",
    "vox_coords.reshape(2,2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(sh)\n",
    "b.flatten().view(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11, 12, 13, 14, 15]],\n",
       "\n",
       "         [[16, 17, 18, 19, 20, 21, 22, 23],\n",
       "          [24, 25, 26, 27, 28, 29, 30, 31]]],\n",
       "\n",
       "\n",
       "        [[[32, 33, 34, 35, 36, 37, 38, 39],\n",
       "          [40, 41, 42, 43, 44, 45, 46, 47]],\n",
       "\n",
       "         [[48, 49, 50, 51, 52, 53, 54, 55],\n",
       "          [56, 57, 58, 59, 60, 61, 62, 63]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0,64)\n",
    "a = a.reshape(2,2,2,8)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]],\n",
       "\n",
       "        [[48, 49, 50, 51],\n",
       "         [52, 53, 54, 55],\n",
       "         [56, 57, 58, 59],\n",
       "         [60, 61, 62, 63]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(4,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2, -1],\n",
      "         [ 0,  1]],\n",
      "\n",
      "        [[ 2,  3],\n",
      "         [ 4,  5]]])\n"
     ]
    }
   ],
   "source": [
    "b = (a-2)\n",
    "print(b)\n",
    "x,y,z = torch.where(b>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x)):\n",
    "    print(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "sigma = 0.02\n",
    "a = torch.distributions.normal.Normal(torch.tensor(0.), torch.tensor(sigma/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[      1,       2,       3, 1000000, 1000000],\n",
       "        [      1,       2,       3, 1000000, 1000000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INF = 1e6\n",
    "dp_mask = torch.tensor([[1,2,3,0,0],[1,2,3,0,0]])\n",
    "\n",
    "dp_mask.masked_fill_(dp_mask.eq(0.0), INF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59.8413],\n",
       "        [59.8413],\n",
       "        [59.8413]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log_prob(b).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59.8413)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log_prob(0).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6647772617907017"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "func = lambda x: math.exp(-(x**2)/(2*(sigma/3)**2))/(math.sqrt(2*math.pi)*(sigma/3))\n",
    "func(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vis the weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rgb', 'depth', 'weights', 't_vals'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rout = torch.load(\"vis_dp.pt\",map_location='cpu')\n",
    "rout.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3\n",
    "base = np.random.randint(32000)\n",
    "dp = torch.load('dp.pt')[base:base+num]\n",
    "we = rout['weights'].detach()[base:base+num]\n",
    "tv = rout['t_vals'].detach()[base:base+num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for i in range(num):\n",
    "    tv_mk = tv[i].ne(0.0)\n",
    "    tvs = tv[i][tv_mk]\n",
    "    wes = we[i][tv_mk]\n",
    "    plt.plot(tvs,wes)\n",
    "    \n",
    "    plt.scatter(dp[i],np.array(0.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = torch.load('dp.pt')\n",
    "dp_mask = dp.ne(0.0)\n",
    "dp = dp[dp_mask]\n",
    "we = rout['weights'].detach()[dp_mask]\n",
    "tv = rout['t_vals'].detach()[dp_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 10,  9,  ..., 12, 13, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_max_idx = torch.argmax(tv,dim=-1)\n",
    "tv_max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30445])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we_all = []\n",
    "for index,i in enumerate(tv_max_idx):\n",
    "    we_all.append(we[index][i])\n",
    "we_all = torch.stack(we_all)\n",
    "we_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30445])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_all = []\n",
    "for index,i in enumerate(tv_max_idx):\n",
    "    tv_all.append(tv[index][i])\n",
    "tv_all = torch.stack(tv_all)\n",
    "tv_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5664)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = (tv_all-dp)\n",
    "dists.max()-dists.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T08:10:26.545818Z",
     "start_time": "2022-02-19T08:10:26.545806Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.hist(dists[:100], bins=10)\n",
    "plt.grid(linestyle=\"--\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T08:10:35.421478Z",
     "start_time": "2022-02-19T08:10:35.416069Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afc46cdbaec78713ab9233e91f9bf86eb4111f2df6c8364fa345446d20f0faae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
